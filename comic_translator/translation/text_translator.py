"""
Text Translator
æ–‡å­—ç¿»è­¯å™¨

ä½¿ç”¨Geminié€²è¡Œæ–‡å­—ç¿»è­¯ï¼Œæ”¯æ´æ­·å²ä¸Šä¸‹æ–‡å’Œåœ–ç‰‡åˆ†æ
"""

import json
import re
import time
from typing import List, Dict, Any, Optional
from ..utils.gemini_client import GeminiClient
from pathlib import Path


class TextTranslator:
    """
    æ–‡å­—ç¿»è­¯å™¨
    Text Translator
    
    ä½¿ç”¨Geminié€²è¡Œæ™ºèƒ½ç¿»è­¯ï¼Œæ”¯æ´æ­·å²ä¸Šä¸‹æ–‡å’Œåœ–ç‰‡åˆ†æ
    """
    
    def __init__(self, gemini_client: GeminiClient):
        """
        åˆå§‹åŒ–ç¿»è­¯å™¨
        
        Args:
            gemini_client: Geminiå®¢æˆ¶ç«¯å¯¦ä¾‹
        """
        self.gemini_client = gemini_client
        self.api_call_count = 0
        self.translation_cache = {}
        
        print("âœ… æ–‡å­—ç¿»è­¯å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def translate_texts_with_history(
        self, 
        texts: List[str], 
        terminology_dict: Dict[str, str] = None,
        translation_history: List[Dict[str, str]] = None,
        image_path: str = None
    ) -> Dict[str, Any]:
        """
        ç¿»è­¯æ–‡å­—åˆ—è¡¨ï¼ˆä½¿ç”¨æ­·å²ä¸Šä¸‹æ–‡å’Œåœ–ç‰‡åˆ†æï¼‰
        
        Args:
            texts: å¾…ç¿»è­¯çš„æ–‡å­—åˆ—è¡¨
            terminology_dict: å°ˆæœ‰åè©å­—å…¸
            translation_history: ç¿»è­¯æ­·å²ä¸Šä¸‹æ–‡
            image_path: åœ–ç‰‡è·¯å¾‘ï¼Œç”¨æ–¼è¦–è¦ºåˆ†æï¼ˆå¿…é ˆæä¾›ï¼‰
            
        Returns:
            Dict: ç¿»è­¯çµæœ
        """
        if not texts:
            return {'translated_texts': [], 'new_terminology': {}, 'success': False}
        
        if not image_path or not Path(image_path).exists():
            print("âŒ æ‰€æœ‰ç¿»è­¯éƒ½å¿…é ˆæä¾›æœ‰æ•ˆçš„åœ–ç‰‡è·¯å¾‘")
            return {'translated_texts': [], 'new_terminology': {}, 'success': False}
        
        print(f"ğŸŒ é–‹å§‹ç¿»è­¯ {len(texts)} å€‹æ–‡å­—...")
        print(f"ğŸ“· ä½¿ç”¨åœ–ç‰‡åˆ†æ: {Path(image_path).name}")
        
        # ä½¿ç”¨structured outputé€²è¡Œç¿»è­¯
        response_schema = {
            "type": "object",
            "properties": {
                "translations": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "original": {"type": "string"},
                            "translated": {"type": "string"},
                            "text_direction": {
                                "type": "string",
                                "enum": ["horizontal", "vertical"],
                                "description": "æ–‡å­—æ’ç‰ˆæ–¹å‘ï¼šhorizontal(æ©«æ›¸)æˆ–vertical(ç›´æ›¸)"
                            },
                            "bubble_type": {
                                "type": "string", 
                                "enum": ["pure_white", "textured", "transparent"],
                                "description": "å°è©±æ¡†é¡å‹ï¼špure_white(ç´”ç™½)ã€textured(æœ‰ç´‹ç†/æ¼¸è®Š)æˆ–transparent(é€æ˜)"
                            },
                            "estimated_font_size": {
                                "type": "integer",
                                "description": "ä¼°è¨ˆçš„åŸå§‹å­—é«”å¤§å°(åƒç´ )"
                            }
                        },
                        "required": ["original", "translated", "text_direction", "bubble_type", "estimated_font_size"]
                    }
                },
                "new_terminology": {
                    "type": "array",
                    "description": "ç™¼ç¾çš„æ–°å°ˆæœ‰åè©åˆ—è¡¨",
                    "items": {
                        "type": "object",
                        "properties": {
                            "japanese": {"type": "string", "description": "æ—¥æ–‡åŸæ–‡"},
                            "chinese": {"type": "string", "description": "ä¸­æ–‡ç¿»è­¯"}
                        },
                        "required": ["japanese", "chinese"]
                    }
                }
            },
            "required": ["translations", "new_terminology"]
        }
        
        prompt = self._create_translation_prompt(texts, terminology_dict, translation_history, image_path)
        
        try:
            result = self.gemini_client.generate_structured_content_with_image(
                prompt, image_path, response_schema
            )
            print("âœ… ä½¿ç”¨åœ–ç‰‡è¦–è¦ºåˆ†æé€²è¡Œç¿»è­¯")
            
            translations = result.get('translations', [])
            new_terminology = result.get('new_terminology', [])
            
            # å°‡new_terminologyå¾arrayè½‰æ›ç‚ºdictæ ¼å¼
            terminology_dict = {}
            for term in new_terminology:
                if isinstance(term, dict) and 'japanese' in term and 'chinese' in term:
                    terminology_dict[term['japanese']] = term['chinese']
            
            # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
            translated_texts = []
            for translation in translations:
                translated_texts.append({
                    'original': translation['original'],
                    'translated': translation['translated'],
                    'text_direction': translation['text_direction'],
                    'bubble_type': translation['bubble_type'],
                    'estimated_font_size': translation['estimated_font_size']
                })
            
            # é©—è­‰è¼¸å‡ºæ•¸é‡
            if len(translated_texts) != len(texts):
                print(f"âš ï¸ ç¿»è­¯æ•¸é‡ä¸åŒ¹é…: è¼¸å…¥ {len(texts)} å€‹ï¼Œè¼¸å‡º {len(translated_texts)} å€‹")
                print("ğŸ“ AIå¯èƒ½åˆªé™¤äº†æŸäº›æ®µè½ï¼Œå°‡ä½¿ç”¨ç°¡å–®ç¿»è­¯æ–¹æ³•")
                return self._simple_translation(texts, terminology_dict, translation_history, image_path)
            
            print(f"âœ… ç¿»è­¯å®Œæˆ: {len(translated_texts)} å€‹æ–‡å­—")
            
            return {
                'translated_texts': translated_texts,
                'new_terminology': terminology_dict,
                'success': True
            }
            
        except Exception as e:
            print(f"âš ï¸ Structuredç¿»è­¯å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®æ–¹æ³•: {e}")
            return self._simple_translation(texts, terminology_dict, translation_history, image_path)
    
    def _create_translation_prompt(self, texts: List[str], terminology_dict: Dict[str, str], 
                                  translation_history: List[Dict[str, str]] = None, 
                                  image_path: str = None) -> str:
        """
        å‰µå»ºç¿»è­¯æç¤ºè©
        
        Args:
            texts: å¾…ç¿»è­¯æ–‡å­—
            terminology_dict: å°ˆæœ‰åè©å­—å…¸
            translation_history: ç¿»è­¯æ­·å²
            image_path: åœ–ç‰‡è·¯å¾‘
            
        Returns:
            str: ç¿»è­¯æç¤ºè©
        """
        # æº–å‚™æ­·å²ä¸Šä¸‹æ–‡
        history_context = ""
        if translation_history and len(translation_history) > 0:
            recent_history = translation_history[-10:]  # æœ€è¿‘10æ¢
            history_context = "\n\nå‰æ–‡ç¿»è­¯åƒè€ƒï¼š\n"
            for i, item in enumerate(recent_history, 1):
                history_context += f"{i}. ã€Œ{item.get('original', '')}ã€â†’ã€Œ{item.get('translated', '')}ã€\n"
        
        # æº–å‚™å°ˆæœ‰åè©
        terminology_context = ""
        if terminology_dict:
            terminology_context = "\n\nå°ˆæœ‰åè©å­—å…¸ï¼ˆå¿…é ˆä½¿ç”¨ä¸€è‡´ç¿»è­¯ï¼‰ï¼š\n"
            for jp, cn in terminology_dict.items():
                terminology_context += f"ã€Œ{jp}ã€â†’ã€Œ{cn}ã€\n"

        return f"""ä½ æ˜¯å°ˆæ¥­çš„æ—¥æ–‡æ¼«ç•«ç¿»è­¯å°ˆå®¶å’Œ OCR æ ¡æ­£å°ˆå®¶ã€‚è«‹åˆ†ææä¾›çš„ OCR è­˜åˆ¥æ–‡å­—ï¼Œæ ¡æ­£å¯èƒ½çš„éŒ¯èª¤ï¼Œç„¶å¾Œå°‡æ­£ç¢ºçš„æ—¥æ–‡ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ã€‚

**é‡è¦ï¼šè«‹åŒæ™‚åˆ†ææä¾›çš„æ¼«ç•«åœ–ç‰‡ï¼Œé€²è¡ŒOCRæ ¡æ­£å’Œè¦–è¦ºç‰¹å¾µåˆ†æ**

**åš´æ ¼è¦å‰‡ï¼šçµ•å°ä¸å¯åˆªé™¤ä»»ä½•æ®µè½ï¼**
- å¿…é ˆç‚ºæ¯å€‹æä¾›çš„ OCR æ–‡å­—æä¾›å°æ‡‰çš„æ ¡æ­£å’Œç¿»è­¯çµæœ
- å³ä½¿ OCR æ–‡å­—çœ‹èµ·ä¾†æœ‰å•é¡Œï¼Œä¹Ÿè¦å˜—è©¦ç†è§£ä¸¦æ ¡æ­£ï¼Œä¸å¯è·³éæˆ–åˆªé™¤
- è¼¸å‡ºçš„ç¿»è­¯æ•¸é‡å¿…é ˆèˆ‡è¼¸å…¥çš„ OCR æ–‡å­—æ•¸é‡å®Œå…¨ä¸€è‡´

åœ–ç‰‡åˆ†æè¦æ±‚ï¼š
1. **OCR æ ¡æ­£åŠŸèƒ½**ï¼š
   - å°‡æä¾›çš„ OCR è­˜åˆ¥æ–‡å­—èˆ‡åœ–ç‰‡ä¸­çš„å¯¦éš›æ–‡å­—é€²è¡Œå°æ¯”
   - è­˜åˆ¥å¯èƒ½çš„ OCR éŒ¯èª¤ï¼ˆå¦‚ç›¸ä¼¼å­—ç¬¦æ··æ·†ã€ç¼ºå­—ã€å¤šå­—ç­‰ï¼‰
   - æ ¡æ­£éŒ¯èª¤çš„æ–‡å­—ï¼Œç¢ºä¿æº–ç¢ºç†è§£åŸæ–‡å«ç¾©
   - å¸¸è¦‹ OCR éŒ¯èª¤ç¤ºä¾‹ï¼š
     * ã€Œãƒ­ã€èˆ‡ã€Œå£ã€ã€ã€ŒåŠ›ã€èˆ‡ã€Œåˆ€ã€ã€ã€Œãƒ¼ã€èˆ‡ã€Œä¸€ã€çš„æ··æ·†
     * æ‰‹å¯«å­—é«”å¯èƒ½å°è‡´çš„è­˜åˆ¥éŒ¯èª¤
     * ç‰¹æ®Šå­—é«”ã€æ–œé«”æ–‡å­—çš„è­˜åˆ¥å•é¡Œ
   - **æ³¨æ„**ï¼šå³ä½¿ç„¡æ³•å®Œå…¨ç†è§£ OCR æ–‡å­—ï¼Œä¹Ÿè¦æä¾›æœ€åˆç†çš„æ ¡æ­£ç‰ˆæœ¬

2. **å¯¦éš›è§€å¯Ÿæ–‡å­—æ’ç‰ˆ**ï¼š
   - ä»”ç´°è§€å¯Ÿæ¯å€‹æ–‡å­—å€åŸŸçš„æ–‡å­—æ˜¯æ©«å‘æ’åˆ—é‚„æ˜¯ç¸±å‘æ’åˆ—
   - horizontalï¼šæ–‡å­—å¾å·¦åˆ°å³æ°´å¹³æ’åˆ—
   - verticalï¼šæ–‡å­—å¾ä¸Šåˆ°ä¸‹å‚ç›´æ’åˆ—
   
   **æ¼«ç•«ä¸­å¸¸è¦‹çš„æ’ç‰ˆè¦å¾‹**ï¼š
   - **å°è©±æ¡†å…§çš„é•·å¥å­**ï¼šé€šå¸¸æ˜¯ verticalï¼ˆç›´æ›¸ï¼‰ï¼Œæ–‡å­—å¾ä¸Šåˆ°ä¸‹ã€å¾å³åˆ°å·¦æ’åˆ—
   - **çŸ­å¥æ„Ÿå˜†è©**ï¼šå¯èƒ½æ˜¯ horizontalï¼ˆæ©«æ›¸ï¼‰ï¼Œç‰¹åˆ¥æ˜¯å¾ˆçŸ­çš„è©èª
   - **æ—ç™½èªªæ˜æ–‡å­—**ï¼šé€šå¸¸æ˜¯ verticalï¼ˆç›´æ›¸ï¼‰
   - **éŸ³æ•ˆæ–‡å­—**ï¼šé€šå¸¸æ˜¯ horizontalï¼ˆæ©«æ›¸ï¼‰ï¼Œä½†è¦çœ‹å¯¦éš›æ’åˆ—æ–¹å‘
   - **æ¨™é¡Œæˆ–å¤§å­—**ï¼šå¯èƒ½æ˜¯ horizontalï¼ˆæ©«æ›¸ï¼‰
   
   **åˆ¤æ–·æŠ€å·§**ï¼š
   - è§€å¯Ÿæ–‡å­—åœ¨å°è©±æ¡†ä¸­çš„å¯¦éš›æ’åˆ—æ–¹å‘ï¼Œè€Œä¸æ˜¯å°è©±æ¡†çš„å½¢ç‹€
   - é•·å¥å­ï¼ˆè¶…é5å€‹å­—ç¬¦ï¼‰åœ¨æ¼«ç•«ä¸­é€šå¸¸æ¡ç”¨ verticalï¼ˆç›´æ›¸ï¼‰
   - å¦‚æœæ–‡å­—çœ‹èµ·ä¾†æ˜¯ä¸€è¡Œä¸€è¡Œå¾ä¸Šåˆ°ä¸‹æ’åˆ—ï¼Œå°±æ˜¯ vertical
   - å¦‚æœæ–‡å­—çœ‹èµ·ä¾†æ˜¯å¾å·¦åˆ°å³åœ¨åŒä¸€æ°´å¹³ç·šä¸Šï¼Œå°±æ˜¯ horizontal

3. **å°è©±æ¡†èƒŒæ™¯åˆ†æ**ï¼š
   - pure_whiteï¼šç´”ç™½è‰²èƒŒæ™¯ï¼Œé‚Šç·£æ¸…æ™°çš„å°è©±æ¡†
   - texturedï¼šæœ‰æ¼¸è®Šã€é™°å½±ã€ç´‹ç†çš„å°è©±æ¡†èƒŒæ™¯
   - transparentï¼šé€æ˜æˆ–åŠé€æ˜çš„å°è©±æ¡†

4. **å­—é«”å¤§å°ä¼°è¨ˆ**ï¼š
   - æ ¹æ“šåœ–ç‰‡ä¸­æ–‡å­—çš„å¯¦éš›å¤§å°ä¼°è¨ˆåƒç´ å€¼ï¼ˆé€šå¸¸8-40åƒç´ ï¼‰
   - è€ƒæ…®æ–‡å­—èˆ‡å°è©±æ¡†çš„æ¯”ä¾‹é—œä¿‚

ç¿»è­¯å’Œæ ¡æ­£åŸå‰‡ï¼š
- **çµ•å°ç¦æ­¢**ï¼šåˆªé™¤ä»»ä½•è¼¸å…¥çš„æ–‡å­—æ®µè½ï¼å¿…é ˆç‚ºæ¯å€‹è¼¸å…¥æä¾›å°æ‡‰çš„è¼¸å‡º
- **é¦–è¦ä»»å‹™**ï¼šæ ¡æ­£ OCR è­˜åˆ¥éŒ¯èª¤ï¼Œç¢ºä¿ç†è§£æ­£ç¢ºçš„åŸæ–‡
- ä¿æŒæ¼«ç•«å°è©±çš„è‡ªç„¶æ€§å’Œæµæš¢æ€§
- ç¶­æŒè§’è‰²çš„èªèª¿å’Œå€‹æ€§
- å°ˆæœ‰åè©å¿…é ˆä¿æŒä¸€è‡´æ€§
- è€ƒæ…®ä¸Šä¸‹æ–‡é€£è²«æ€§

**é—œéµç¿»è­¯é‚è¼¯**ï¼š
1. **æª¢æŸ¥å°ˆæœ‰åè©å­—å…¸**ï¼šæŸ¥çœ‹å­—å…¸ä¸­æ˜¯å¦æœ‰è©²è©å½™çš„ç¿»è­¯
2. **æ•¬èªç¿»è­¯è¦å‰‡**ï¼š
   - å¦‚æœå­—å…¸ä¸­æœ‰ã€Œäººå(ç”·æ€§)ã€ï¼Œå‰‡ã€Œäººåã•ã‚“ã€ç¿»è­¯ç‚ºã€Œäººåå…ˆç”Ÿã€
   - å¦‚æœå­—å…¸ä¸­æœ‰ã€Œäººå(å¥³æ€§)ã€ï¼Œå‰‡ã€Œäººåã•ã‚“ã€ç¿»è­¯ç‚ºã€Œäººåå°å§ã€
   - ä¾‹å¦‚ï¼šå­—å…¸ä¸­ã€Œã‚­ã‚¯ãƒ«(ç”·æ€§)ã€â†’ã€Œã‚­ã‚¯ãƒ«ã•ã‚“ã€ç¿»è­¯ç‚ºã€Œå¥‡åº«é­¯å…ˆç”Ÿã€
   - ä¾‹å¦‚ï¼šå­—å…¸ä¸­ã€Œã‚¨ãƒãƒ¡(å¥³æ€§)ã€â†’ã€Œã‚¨ãƒãƒ¡ã•ã‚“ã€ç¿»è­¯ç‚ºã€Œè‰¾è«¾æ¢…å°å§ã€
3. **æ–°è§’è‰²è™•ç†**ï¼šå¦‚æœå­—å…¸ä¸­æ²’æœ‰è©²è§’è‰²ï¼Œæ ¹æ“šåœ–ç‰‡å’Œä¸Šä¸‹æ–‡åˆ¤æ–·æ€§åˆ¥ï¼Œç„¶å¾Œæ­£ç¢ºç¿»è­¯æ•¬èªï¼Œä¸¦åœ¨new_terminologyä¸­è¨˜éŒ„ç‚ºã€Œäººå(æ€§åˆ¥)ã€

å¾…è™•ç†çš„ OCR è­˜åˆ¥æ–‡å­—ï¼š
{json.dumps(texts, ensure_ascii=False, indent=2)}
{terminology_context}
{history_context}

**é‡è¦ç´„æŸï¼šè¼¸å‡ºçš„ç¿»è­¯æ•¸é‡å¿…é ˆç­‰æ–¼è¼¸å…¥æ–‡å­—æ•¸é‡ ({len(texts)} å€‹)**

è«‹ç‚ºæ¯å€‹æ–‡å­—æä¾›ï¼š
1. **æ ¡æ­£å¾Œçš„æ­£ç¢ºæ—¥æ–‡åŸæ–‡**ï¼ˆå¦‚æœ OCR æœ‰éŒ¯èª¤ï¼‰
2. æº–ç¢ºçš„ç¹é«”ä¸­æ–‡ç¿»è­¯ï¼ˆæ³¨æ„æ€§åˆ¥æ­£ç¢ºæ€§ï¼‰
3. æ–‡å­—æ’ç‰ˆæ–¹å‘åˆ¤æ–·ï¼ˆhorizontal/verticalï¼‰
4. å°è©±æ¡†é¡å‹åˆ¤æ–·ï¼ˆpure_white/textured/transparentï¼‰
5. ä¼°è¨ˆçš„å­—é«”å¤§å°ï¼ˆåƒç´ å€¼ï¼‰
6. ç™¼ç¾çš„æ–°å°ˆæœ‰åè©ï¼ˆæ ¼å¼ï¼šæ—¥æ–‡->ä¸­æ–‡ï¼Œå¦‚æœæ˜¯äººåè«‹åœ¨ä¸­æ–‡å¾ŒåŠ (ç”·æ€§)æˆ–(å¥³æ€§)ï¼‰

æ³¨æ„ï¼šåœ¨ "original" æ¬„ä½ä¸­è«‹æä¾›æ ¡æ­£å¾Œçš„æ­£ç¢ºæ—¥æ–‡ï¼Œè€Œä¸æ˜¯ OCR çš„éŒ¯èª¤è­˜åˆ¥çµæœã€‚

è¼¸å‡ºå¿…é ˆæ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"""
    
    def _simple_translation(self, texts: List[str], terminology_dict: Dict[str, str], 
                           translation_history: List[Dict[str, str]] = None, 
                           image_path: str = None) -> Dict[str, Any]:
        """
        ç°¡å–®ç¿»è­¯æ–¹æ³•ï¼ˆç•¶structuredæ–¹æ³•å¤±æ•—æ™‚ä½¿ç”¨ï¼‰
        
        Args:
            texts: è¦ç¿»è­¯çš„æ–‡å­—åˆ—è¡¨
            terminology_dict: å°ˆæœ‰åè©å­—å…¸
            translation_history: ç¿»è­¯æ­·å²ä¸Šä¸‹æ–‡
            image_path: åœ–ç‰‡è·¯å¾‘
            
        Returns:
            Dict: ç¿»è­¯çµæœ
        """
        if not texts:
            return {
                'translated_texts': [],
                'new_terminology': [],
                'success': False
            }
        
        print(f"ğŸ”„ é–‹å§‹ç°¡å–®ç¿»è­¯ {len(texts)} å€‹æ–‡å­—")
        if translation_history:
            print(f"ğŸ“š ä½¿ç”¨ {len(translation_history)} æ¢æ­·å²ç¿»è­¯ä½œç‚ºä¸Šä¸‹æ–‡")
        print(f"ğŸ“· ä½¿ç”¨åœ–ç‰‡é€²è¡Œ OCR æ ¡æ­£: {Path(image_path).name}")
        
        start_time = time.time()
        
        try:
            # æº–å‚™ç¿»è­¯æç¤ºè©ï¼ŒåŒ…å«æ­·å²ä¸Šä¸‹æ–‡
            prompt = self._create_simple_prompt(texts, terminology_dict or {}, translation_history or [], image_path)
            
            # å‘¼å«Gemini API
            self.api_call_count += 1
            print(f"ğŸ’° API å‘¼å« #{self.api_call_count} - æ¨¡å‹: {self.gemini_client.model_name}")
            
            response = self.gemini_client.generate_content_with_image(prompt, image_path)
            print("âœ… ä½¿ç”¨åœ–ç‰‡é€²è¡Œ OCR æ ¡æ­£ç¿»è­¯")
            
            if not response:
                print("âŒ API å›æ‡‰ç‚ºç©º")
                return {
                    'translated_texts': self._create_fallback_translations(texts),
                    'new_terminology': [],
                    'success': False,
                    'error': 'Empty API response'
                }
            
            # è§£æç¿»è­¯çµæœ
            result = self._parse_simple_response(response, texts)
            
            processing_time = time.time() - start_time
            
            print(f"âœ… ç¿»è­¯å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’")
            print(f"ğŸ“ æˆåŠŸç¿»è­¯: {len(result['translated_texts'])}/{len(texts)}")
            print(f"ğŸ†• ç™¼ç¾æ–°å°ˆæœ‰åè©: {len(result['new_terminology'])}")
            
            return {
                'translated_texts': result['translated_texts'],
                'new_terminology': result['new_terminology'],
                'success': True
            }
            
        except Exception as e:
            print(f"âŒ ç¿»è­¯éç¨‹å‡ºéŒ¯: {e}")
            return {
                'translated_texts': self._create_fallback_translations(texts),
                'new_terminology': [],
                'success': False,
                'error': str(e)
            }
    
    def _create_simple_prompt(self, texts: List[str], terminology_dict: Dict[str, str],
                             translation_history: List[Dict[str, str]], image_path: str) -> str:
        """
        å‰µå»ºç°¡å–®ç¿»è­¯æç¤ºè©
        
        Args:
            texts: è¦ç¿»è­¯çš„æ–‡å­—
            terminology_dict: å°ˆæœ‰åè©å­—å…¸
            translation_history: ç¿»è­¯æ­·å²
            image_path: åœ–ç‰‡è·¯å¾‘
            
        Returns:
            str: ç¿»è­¯æç¤ºè©
        """
        base_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ—¥æ–‡æ¼«ç•«ç¿»è­¯å¸«å’Œ OCR æ ¡æ­£å°ˆå®¶ï¼Œæ“…é•·å°‡æ—¥æ–‡æ¼«ç•«å°è©±ç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚

**åš´æ ¼è¦å‰‡ï¼šçµ•å°ä¸å¯åˆªé™¤ä»»ä½•æ®µè½ï¼**
- å¿…é ˆç‚ºæ¯å€‹æä¾›çš„ OCR æ–‡å­—æä¾›å°æ‡‰çš„æ ¡æ­£å’Œç¿»è­¯çµæœ
- å³ä½¿ OCR æ–‡å­—çœ‹èµ·ä¾†æœ‰å•é¡Œï¼Œä¹Ÿè¦å˜—è©¦ç†è§£ä¸¦æ ¡æ­£ï¼Œä¸å¯è·³éæˆ–åˆªé™¤
- è¼¸å‡ºçš„ç¿»è­¯æ•¸é‡å¿…é ˆèˆ‡è¼¸å…¥çš„ OCR æ–‡å­—æ•¸é‡å®Œå…¨ä¸€è‡´

**é‡è¦ï¼šOCR æ ¡æ­£åŠŸèƒ½**
è«‹é¦–å…ˆè§€å¯Ÿæä¾›çš„æ¼«ç•«åœ–ç‰‡ï¼Œå°æ¯” OCR è­˜åˆ¥çš„æ–‡å­—èˆ‡åœ–ç‰‡ä¸­çš„å¯¦éš›æ–‡å­—ï¼š
1. è­˜åˆ¥å¯èƒ½çš„ OCR éŒ¯èª¤ï¼ˆå¦‚ç›¸ä¼¼å­—ç¬¦æ··æ·†ã€ç¼ºå­—ã€å¤šå­—ç­‰ï¼‰
2. æ ¡æ­£éŒ¯èª¤çš„æ–‡å­—ï¼Œç¢ºä¿æº–ç¢ºç†è§£åŸæ–‡å«ç¾©
3. å¸¸è¦‹ OCR éŒ¯èª¤ï¼šã€Œãƒ­ã€èˆ‡ã€Œå£ã€ã€ã€ŒåŠ›ã€èˆ‡ã€Œåˆ€ã€ã€ã€Œãƒ¼ã€èˆ‡ã€Œä¸€ã€çš„æ··æ·†ç­‰
4. **æ³¨æ„**ï¼šå³ä½¿ç„¡æ³•å®Œå…¨ç†è§£ OCR æ–‡å­—ï¼Œä¹Ÿè¦æä¾›æœ€åˆç†çš„æ ¡æ­£ç‰ˆæœ¬

**æ–‡å­—æ’ç‰ˆæ–¹å‘åˆ¤æ–·**ï¼š
- ä»”ç´°è§€å¯Ÿåœ–ç‰‡ä¸­æ¯å€‹æ–‡å­—å€åŸŸçš„å¯¦éš›æ’åˆ—æ–¹å‘
- **æ¼«ç•«å¸¸è¦‹è¦å¾‹**ï¼š
  * å°è©±æ¡†å…§çš„é•·å¥å­ï¼šé€šå¸¸æ˜¯ç›´æ›¸ï¼ˆå¾ä¸Šåˆ°ä¸‹æ’åˆ—ï¼‰
  * çŸ­å¥æ„Ÿå˜†è©ï¼šå¯èƒ½æ˜¯æ©«æ›¸
  * éŸ³æ•ˆæ–‡å­—ï¼šé€šå¸¸æ˜¯æ©«æ›¸ï¼Œä½†è¦çœ‹å¯¦éš›æ–¹å‘
- **åˆ¤æ–·æ–¹æ³•**ï¼š
  * é•·å¥å­ï¼ˆè¶…é5å€‹å­—ï¼‰åœ¨æ¼«ç•«ä¸­é€šå¸¸æ˜¯ç›´æ›¸
  * è§€å¯Ÿæ–‡å­—å¯¦éš›æ’åˆ—ï¼Œä¸è¦è¢«å°è©±æ¡†å½¢ç‹€èª¤å°
  * ä¸€è¡Œä¸€è¡Œå¾ä¸Šåˆ°ä¸‹ = ç›´æ›¸
  * å¾å·¦åˆ°å³åœ¨åŒä¸€æ°´å¹³ç·š = æ©«æ›¸

**å°ˆæœ‰åè©è™•ç†**ï¼š
- æª¢æŸ¥å­—å…¸ä¸­æ˜¯å¦æœ‰è©²è©å½™çš„ç¿»è­¯
- å¦‚æœå­—å…¸ä¸­æœ‰ã€Œäººå(ç”·æ€§)ã€ï¼Œå‰‡ã€Œäººåã•ã‚“ã€ç¿»è­¯ç‚ºã€Œäººåå…ˆç”Ÿã€
- å¦‚æœå­—å…¸ä¸­æœ‰ã€Œäººå(å¥³æ€§)ã€ï¼Œå‰‡ã€Œäººåã•ã‚“ã€ç¿»è­¯ç‚ºã€Œäººåå°å§ã€
- æ–°ç™¼ç¾çš„äººåè«‹åœ¨ä¸­æ–‡å¾Œæ¨™è¨˜(ç”·æ€§)æˆ–(å¥³æ€§)

ç¿»è­¯åŸå‰‡ï¼š
1. **çµ•å°ç¦æ­¢**ï¼šåˆªé™¤ä»»ä½•è¼¸å…¥çš„æ–‡å­—æ®µè½ï¼å¿…é ˆç‚ºæ¯å€‹è¼¸å…¥æä¾›å°æ‡‰çš„è¼¸å‡º
2. **é¦–è¦ä»»å‹™**ï¼šæ ¡æ­£ OCR è­˜åˆ¥éŒ¯èª¤ï¼Œç¢ºä¿ç†è§£æ­£ç¢ºçš„åŸæ–‡
3. ä¿æŒæ¼«ç•«å°è©±çš„è‡ªç„¶èªèª¿å’Œæƒ…æ„Ÿ
4. ä½¿ç”¨å°ç£å¸¸ç”¨çš„ç¹é«”ä¸­æ–‡è¡¨é”æ–¹å¼
5. ä¿ç•™è§’è‰²çš„èªªè©±ç‰¹è‰²å’Œå€‹æ€§
6. é©ç•¶è™•ç†æ“¬è²è©å’Œæ„Ÿå˜†è©
7. ç¢ºä¿ç¿»è­¯ç¬¦åˆæ¼«ç•«çš„èªå¢ƒå’Œæ°›åœ
8. ç™¼ç¾ä¸¦æ¨™è¨˜æ–°çš„å°ˆæœ‰åè©ï¼ˆäººåã€åœ°åã€ç‰¹æ®Šè¡“èªç­‰ï¼‰

è¼¸å‡ºæ ¼å¼ï¼š
è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼ˆæ³¨æ„ï¼šåœ¨ç¬¬ä¸€è¡Œè«‹æä¾›æ ¡æ­£å¾Œçš„æ­£ç¢ºæ—¥æ–‡ï¼‰ï¼š
1. æ ¡æ­£å¾Œçš„æ­£ç¢ºæ—¥æ–‡ â†’ ç¿»è­¯çµæœ
2. æ ¡æ­£å¾Œçš„æ­£ç¢ºæ—¥æ–‡ â†’ ç¿»è­¯çµæœ
...

å¦‚æœç™¼ç¾æ–°å°ˆæœ‰åè©ï¼Œè«‹åœ¨æœ€å¾Œåˆ—å‡ºï¼š
æ–°å°ˆæœ‰åè©ï¼š
- æ—¥æ–‡åŸæ–‡: ä¸­æ–‡ç¿»è­¯
- æ—¥æ–‡åŸæ–‡: ä¸­æ–‡ç¿»è­¯"""

        # æ·»åŠ å°ˆæœ‰åè©å­—å…¸
        if terminology_dict:
            terminology_section = "\n\nç¾æœ‰å°ˆæœ‰åè©å­—å…¸ï¼š\n"
            for jp_term, zh_term in terminology_dict.items():
                terminology_section += f"- {jp_term}: {zh_term}\n"
            base_prompt += terminology_section
        
        # æ·»åŠ ç¿»è­¯æ­·å²ä¸Šä¸‹æ–‡
        if translation_history:
            history_section = "\n\nç¿»è­¯æ­·å²ä¸Šä¸‹æ–‡ï¼ˆç”¨æ–¼ä¿æŒè§’è‰²å’ŒåŠ‡æƒ…çš„ä¸€è‡´æ€§ï¼‰ï¼š\n"
            
            # åªé¡¯ç¤ºæœ€è¿‘çš„20æ¢æ­·å²è¨˜éŒ„ï¼Œé¿å…æç¤ºè©éé•·
            recent_history = translation_history[-20:] if len(translation_history) > 20 else translation_history
            
            for i, hist_item in enumerate(recent_history, 1):
                if isinstance(hist_item, dict) and 'original' in hist_item and 'translated' in hist_item:
                    history_section += f"{i}. {hist_item['original']} â†’ {hist_item['translated']}\n"
            
            history_section += "\nè«‹åƒè€ƒä»¥ä¸Šç¿»è­¯æ­·å²ï¼Œä¿æŒè§’è‰²åç¨±ã€èªªè©±é¢¨æ ¼å’ŒåŠ‡æƒ…çš„ä¸€è‡´æ€§ã€‚"
            base_prompt += history_section
        
        # æ·»åŠ å¾…ç¿»è­¯æ–‡å­—
        texts_section = f"\n\nå¾…æ ¡æ­£å’Œç¿»è­¯çš„ OCR è­˜åˆ¥æ–‡å­—ï¼ˆå…± {len(texts)} å€‹ï¼Œå¿…é ˆå…¨éƒ¨è™•ç†ï¼‰ï¼š\n"
        for i, text in enumerate(texts, 1):
            texts_section += f"{i}. {text}\n"
        
        base_prompt += texts_section
        base_prompt += f"\n\n**é‡è¦æé†’ï¼šå¿…é ˆè¼¸å‡º {len(texts)} å€‹ç¿»è­¯çµæœï¼Œçµ•å°ä¸å¯å°‘æ–¼é€™å€‹æ•¸é‡ï¼**\n\nè«‹é–‹å§‹æ ¡æ­£å’Œç¿»è­¯ï¼š"
        
        return base_prompt
    
    def _parse_simple_response(self, response: str, original_texts: List[str]) -> Dict[str, Any]:
        """
        è§£æç°¡å–®ç¿»è­¯å›æ‡‰
        
        Args:
            response: APIå›æ‡‰
            original_texts: åŸå§‹æ–‡å­—åˆ—è¡¨
            
        Returns:
            Dict: è§£æå¾Œçš„çµæœ
        """
        lines = response.strip().split('\n')
        translated_texts = []
        new_terminology_list = []
        
        # æŸ¥æ‰¾ç¿»è­¯è¡Œå’Œæ–°å°ˆæœ‰åè©
        in_terminology_section = False
        translation_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if 'æ–°å°ˆæœ‰åè©' in line or 'new terminology' in line.lower():
                in_terminology_section = True
                continue
            
            if in_terminology_section:
                # è§£æå°ˆæœ‰åè©
                if ':' in line or 'ï¼š' in line:
                    parts = re.split(r'[:ï¼š]', line, 1)
                    if len(parts) == 2:
                        jp_term = parts[0].strip().lstrip('- ')
                        cn_term = parts[1].strip()
                        if jp_term and cn_term:
                            new_terminology_list.append({
                                'japanese': jp_term,
                                'chinese': cn_term
                            })
            else:
                # è§£æç¿»è­¯è¡Œ
                if 'â†’' in line:
                    translation_lines.append(line)
                elif re.match(r'^\d+\.', line):
                    translation_lines.append(line)
        
        # è½‰æ›new_terminologyç‚ºdictæ ¼å¼
        new_terminology = {}
        for term in new_terminology_list:
            new_terminology[term['japanese']] = term['chinese']
        
        print(f"ğŸ” ç°¡å–®è§£æ: æ‰¾åˆ° {len(translation_lines)} è¡Œç¿»è­¯ï¼Œéœ€è¦ {len(original_texts)} å€‹çµæœ")
        
        for i, original in enumerate(original_texts):
            if i < len(translation_lines):
                line = translation_lines[i]
                # è§£æç¿»è­¯è¡Œ
                if 'â†’' in line:
                    parts = line.split('â†’', 1)
                    if len(parts) == 2:
                        corrected_original = parts[0].strip()
                        # ç§»é™¤æ•¸å­—é–‹é ­
                        corrected_original = re.sub(r'^\d+[\.\)]\s*', '', corrected_original)
                        translated = parts[1].strip()
                    else:
                        corrected_original = original
                        translated = line.strip()
                else:
                    # ç§»é™¤æ•¸å­—é–‹é ­
                    translated = re.sub(r'^\d+[\.\)]\s*', '', line)
                    corrected_original = original
                
                # å¦‚æœç¿»è­¯çµæœç‚ºç©ºæˆ–å¤ªçŸ­ï¼Œä½¿ç”¨åŸæ–‡
                if not translated or len(translated) < 1:
                    translated = original
                    corrected_original = original
            else:
                # å¦‚æœæ²’æœ‰å°æ‡‰ç¿»è­¯ï¼Œä½¿ç”¨åŸæ–‡
                translated = original
                corrected_original = original
                print(f"âš ï¸ ç¬¬ {i+1} å€‹æ–‡å­—æ²’æœ‰æ‰¾åˆ°ç¿»è­¯ï¼Œä½¿ç”¨åŸæ–‡: {original}")
            
            translated_texts.append({
                'original': corrected_original,
                'translated': translated,
                'text_direction': 'horizontal',
                'bubble_type': 'pure_white',
                'estimated_font_size': 16
            })
        
        print(f"âœ… ç°¡å–®è§£æå®Œæˆ: {len(translated_texts)} å€‹çµæœ")
        
        return {
            'translated_texts': translated_texts,
            'new_terminology': new_terminology
        }
    
    def _create_fallback_translations(self, texts: List[str]) -> List[Dict[str, str]]:
        """
        å‰µå»ºå‚™ç”¨ç¿»è­¯çµæœï¼ˆç•¶ç¿»è­¯å¤±æ•—æ™‚ä½¿ç”¨ï¼‰
        
        Args:
            texts: åŸå§‹æ–‡å­—åˆ—è¡¨
            
        Returns:
            List[Dict]: å‚™ç”¨ç¿»è­¯çµæœ
        """
        return [
            {
                'original': text,
                'translated': text,
                'text_direction': 'horizontal',
                'bubble_type': 'pure_white',
                'estimated_font_size': 16
            }
            for text in texts
        ]
    
    def get_translation_stats(self) -> Dict[str, Any]:
        """
        ç²å–ç¿»è­¯çµ±è¨ˆè³‡è¨Š
        
        Returns:
            Dict: çµ±è¨ˆè³‡è¨Š
        """
        return {
            'total_api_calls': self.api_call_count,
            'model_used': self.gemini_client.model,
            'cache_size': len(self.translation_cache)
        } 